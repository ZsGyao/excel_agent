use crate::models::AppConfig;
use crate::services::python;
use anyhow::Result;
use reqwest::{self, Client};
use serde_json::{self, json, Value};
use std::{fs::read_to_string, path::Path}; // ç¡®ä¿ main.rs ä¸­æœ‰ mod services;

/// å†…éƒ¨ helper: è¯»å– Prompt æ¨¡æ¿
fn load_prompt_template(filename: &str) -> String {
    let path = Path::new("assets").join(filename);
    read_to_string(path).unwrap_or_else(|_| {
        println!("âš ï¸ Warning: Prompt file {} not found!", filename);
        // å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¾é  LLM çš„æ³›åŒ–èƒ½åŠ›
        String::new()
    })
}

/// å†…éƒ¨ helper: åŸºç¡€ LLM è°ƒç”¨
async fn llm_request(config: &AppConfig, system_prompt: &str, user_prompt: &str) -> Result<String> {
    let profile = config.active_profile();
    let api_key = &profile.api_key;
    let base_url = &profile.base_url;
    let model = &profile.model_id;

    let client = Client::new();

    // æ„é€ è¯·æ±‚ï¼Œä¿æŒä½æ¸©ä»¥ç¡®ä¿ç¨³å®š
    let response = client
        .post(format!(
            "{}/chat/completions",
            base_url.trim_end_matches('/')
        ))
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .json(&json!({
            "model": model,
            "messages": [
                { "role": "system", "content": system_prompt },
                { "role": "user", "content": user_prompt }
            ],
            "temperature": 0.1
        }))
        .send()
        .await?;

    if !response.status().is_success() {
        return Ok(format!("API Error: {}", response.text().await?));
    }

    let json: Value = response.json().await?;
    Ok(json["choices"][0]["message"]["content"]
        .as_str()
        .unwrap_or("")
        .to_string())
}

/// ä¸»å…¥å£: æ™ºèƒ½ Re-Act å¾ªç¯ (ç”Ÿæˆä»£ç ç‰ˆ)
///
/// é€»è¾‘ï¼š
/// 1. ä¾¦å¯Ÿ (Peek) -> 2. è§„åˆ’ (Plan) -> 3. ç¼–ç  (Code) -> 4. è¿”å›å‰ç«¯ (Return)
/// **æ³¨æ„ï¼šä¸è‡ªåŠ¨æ‰§è¡Œä»£ç ï¼Œäº¤ç”±ç”¨æˆ·ç¡®è®¤ã€‚**
pub async fn call_ai(
    config: &AppConfig,
    user_content: &str,
    context_file_path: Option<String>,
) -> Result<String> {
    // 1. å¦‚æœæ²¡æœ‰æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼Œç›´æ¥è¿›è¡Œæ™®é€šé—²èŠ
    let file_path = match context_file_path {
        Some(path) => path,
        None => {
            // ä½¿ç”¨é»˜è®¤ System Prompt
            let sys_prompt = load_prompt_template("system_prompt.md");
            return llm_request(config, &sys_prompt, user_content).await;
        }
    };

    println!("ğŸš€ å¯åŠ¨ Re-Act ç”Ÿæˆæµç¨‹: {}", file_path);

    // --- STEP 1: æ„ŸçŸ¥ (Peek) ---
    // è°ƒç”¨ Python è·å–å‰ 20 è¡Œæ•°æ®æŒ‡çº¹ï¼Œç”¨äºè¾…åŠ©å†³ç­–
    println!("ğŸ‘€ [Step 1] æ­£åœ¨ä¾¦å¯Ÿ Excel ç»“æ„...");
    let peek_json_str = python::peek_excel(&file_path)
        .await
        .unwrap_or_else(|e| format!("{{'status': 'error', 'msg': '{}'}}", e));

    // --- STEP 2: æ€è€ƒ (Plan) ---
    // è®© LLM åˆ†æè¡¨å¤´ç»“æ„ï¼Œå†³å®š header_count
    println!("ğŸ§  [Step 2] æ­£åœ¨è§„åˆ’ä»»åŠ¡...");
    let planner_tmpl = load_prompt_template("prompt_planner.md");
    let user_msg_plan = format!(
        "User Query: {}\nCSV Preview:\n{}",
        user_content, peek_json_str
    );
    // å¦‚æœæ²¡æœ‰ planner æ¨¡æ¿ï¼Œè·³è¿‡è¿™ä¸€æ­¥ï¼ˆé™çº§å¤„ç†ï¼‰
    let plan_json = if !planner_tmpl.is_empty() {
        llm_request(config, &planner_tmpl, &user_msg_plan).await?
    } else {
        println!("âš ï¸ æœªæ‰¾åˆ° prompt_planner.mdï¼Œè·³è¿‡è§„åˆ’æ­¥éª¤");
        "{}".to_string()
    };
    println!("ğŸ’¡ è§„åˆ’ç»“æœ: {}", plan_json);

    // --- STEP 3: ç¼–ç  (Code) ---
    // æ ¹æ®è§„åˆ’ç»“æœç”Ÿæˆæœ€ç»ˆ Python ä»£ç 
    println!("ğŸ’» [Step 3] æ­£åœ¨ç”Ÿæˆä»£ç ...");
    let coder_tmpl = load_prompt_template("prompt_coder.md");

    // å¦‚æœæ²¡æœ‰ coder æ¨¡æ¿ï¼Œå›é€€åˆ°é»˜è®¤ prompt
    if coder_tmpl.is_empty() {
        let sys_prompt = load_prompt_template("system_prompt.md");
        let fallback_ctx = format!("Target File: {}\nStructure Hint: {}", file_path, plan_json);
        return llm_request(
            config,
            &sys_prompt,
            &format!("{}\n\nContext:\n{}", user_content, fallback_ctx),
        )
        .await;
    }

    let user_msg_code = format!(
        "Structure Config: {}\nUser Query: {}",
        plan_json, user_content
    );

    // æ³¨å…¥æ–‡ä»¶è·¯å¾„
    let coder_tmpl_filled = coder_tmpl.replace("{file_path}", &file_path.replace("\\", "\\\\"));

    let code_response = llm_request(config, &coder_tmpl_filled, &user_msg_code).await?;

    // --- STEP 4: è¿”å› (Return) ---
    // ç›´æ¥è¿”å›ç”Ÿæˆçš„ Markdown ä»£ç å—ã€‚
    // å‰ç«¯ UI ä¼šè¯†åˆ« ```pythonï¼Œå¹¶æ˜¾ç¤ºâ€œè¿è¡Œâ€æŒ‰é’®ã€‚
    println!("âœ… ä»£ç ç”Ÿæˆå®Œæ¯•ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤");

    // å¯é€‰ï¼šåœ¨è¿”å›å†…å®¹å‰åŠ ä¸€ç‚¹åˆ†ææ‘˜è¦ï¼Œè®©ç”¨æˆ·çŸ¥é“ AI æ˜¯æ€ä¹ˆæƒ³çš„
    // let final_response = format!("**åˆ†æå®Œæ¯•**ï¼šæ£€æµ‹åˆ°è¡¨æ ¼ç»“æ„é…ç½®ä¸º `{}`ã€‚\n\n{}", plan_json, code_response);

    // ä¸ºäº†ä¿æŒç•Œé¢ç®€æ´ï¼Œç›´æ¥è¿”å›ä»£ç éƒ¨åˆ†å³å¯ï¼Œæˆ–è€…åªåŒ…å«å¿…è¦çš„è§£é‡Š
    Ok(code_response)
}
